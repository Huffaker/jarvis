python -m pip install -r requirements.txt

Configuration (local, not in git)
  Copy .env.example to .env and set any values for your machine. The app loads .env
  from the project root so you don't need to set environment variables in the shell.

  For image generation: ComfyUI must be running before you use it. Start ComfyUI manually
  (e.g. python main.py from your ComfyUI folder). Optionally set COMFYUI_URL in .env if
  your server uses a different port (default http://127.0.0.1:8188).

Personas (setup)
  The app creates a .personas directory on first run. To add a persona, create a folder
  .personas/<persona_id>/ and put a config file there named <persona_id>.config or config (JSON).

  Example: .personas/assistant/assistant.config

  {
    "name": "Assistant",
    "public": true,
    "system_persona": "You are a helpful AI assistant. You remain accurate, concise, and calm.",
    "decision_model": "qwen3:0.6b",
    "model": "qwen3:4b",
    "vl_model": "qwen3-vl:4b",
    "decisions": {
      "image_generation": true,
      "web_search": true,
      "prior_image_context": true
    },
    "comfyui": {
      "diffusion_model_name": "z-image-turbo-fp8-e4m3fn.safetensors",
      "clip_model_name": "qwen_3_4b.safetensors",
      "vae_model_name": "ae.safetensors"
    }
  }

  Optional: "decisions" toggles LLM-based features (omit or set true/false). Optional: "comfyui"
  for image generation model names.   Optional: "image_gen_system" (string) overrides the default system prompt used to turn the user's
  request into an image-generation prompt. Optional: "image_gen_model" (string) is the Ollama model
  used for that step (defaults to the persona's main "model" if not set).
  Conversation memory is stored in .personas/<persona_id>/memory.json
  and is created automatically when you use that persona.


ComfyUI: run from your ComfyUI install folder so it writes output into this project:
  python main.py --output-directory <path-to-ollama-agent>
Use the full path to this project folder (e.g. C:\Users\you\LLM\ollama-agent or /Users/you/LLM/ollama-agent).

venv\Scripts\activate
python agent_cli.py
python run_when_plugged.py

cudu install:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
verify:
python -c "import torch; print(torch.__version__); print(torch.version.cuda); print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0))"
Expected output:
2.5.1+cu121
12.1
True
NVIDIA GeForce RTX 3080 Laptop GPU


Keeping the app available continuously (Windows)
  When the app is running, it asks Windows to keep the system awake so the
  server stays available. You do not need to change global sleep settings;
  when you stop the app, normal sleep behavior returns.

  To run only when plugged in (stops when unplugged):
    python run_when_plugged.py

  To also keep running with the lid closed when plugged in, set one option:
    Power Options -> Choose what closing the lid does ->
    "When plugged in" -> Do nothing.
